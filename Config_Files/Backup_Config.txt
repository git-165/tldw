[API]
anthropic_api_key = <anthropic_api_key>
anthropic_model = claude-3-5-sonnet-20240620
anthropic_streaming = True
anthropic_temperature = 0.7
#
cohere_api_key = <cohere_api_key>
cohere_model = command-r-plus
cohere_streaming = True
cohere_temperature = 0.7
#
groq_api_key = <groq_api_key>
groq_model = llama3-70b-8192
groq_streaming = True
groq_temperature = 0.7
#
openai_api_key = <openai_api_key>
openai_model = gpt-4o
openai_streaming = False
openai_temperature = 0.7
#
huggingface_api_key = <huggingface_api_key>
huggingface_model = meta-llama/Llama-3.1-70B-Instruct
huggingface_streaming = True
huggingface_temperature = 0.7
#
openrouter_api_key = <openrouter_api_key>
openrouter_model = mistralai/mistral-7b-instruct:free
#
deepseek_api_key = <deepseek_api_key>
deepseek_model = deepseek-chat
deepseek_streaming = True
deepseek_temperature = 0.7
#
mistral_api_key = <mistral_api_key>
mistral_model = mistral-large-latest
mistral_streaming = True
mistral_temperature = 0.7
#
google_api_key = <google_api_key>
# Available Model Options:
google_model = gemini-1.5-pro
google_streaming = True
google_temperature = 0.7
#
elevenlabs_api_key = <eleven_labs_api_key>
#
custom_openai_api_key = <key_here>
custom_openai_api_ip = <api_ip_here>
custom_openai_api_streaming = True
custom_openai_api_temperature = 0.7
custom_openai_api_top_p = 0.9
custom_openai_api_min_p = 0.05
#
default_api = openai


[Local-API]
kobold_api_IP = http://127.0.0.1:5001/api/v1/generate
kobold_openai_api_IP = http://127.0.0.1:5001/v1/chat/completions
kobold_api_key =
kobold_streaming = True
kobold_temperature = 0.7
kobold_top_p = 0.9
kobold_min_p = 0.05
#
llama_api_IP = http://127.0.0.1:8080/completion
llama_api_key =
llama_streaming = True
llama_temperature = 0.7
llama_top_p = 0.9
llama_min_p = 0.05
#
ooba_api_key =
ooba_api_IP = http://127.0.0.1:5000/v1/chat/completions
ooba_streaming = True
ooba_temperature = 0.7
ooba_top_p = 0.9
ooba_min_p = 0.05
#
tabby_api_IP = http://127.0.0.1:5000/v1/chat/completions
tabby_api_key =
tabby_streaming = True
tabby_temperature = 0.7
#
vllm_api_IP = http://127.0.0.1:8000/v1/chat/completions
vllm_model =
vllm_api_key =
vllm_streaming = True
vllm_temperature = 0.7
#
ollama_api_IP = http://127.0.0.1:11434/v1/chat/completions
ollama_api_key =
ollama_model = llama3
ollama_streaming = True
#
aphrodite_api_IP = http://127.0.0.1:8080/completion
aphrodite_api_key =
aphrodite_streaming = True
aphrodite_temperature = 0.7
aphrodite_top_p = 0.9
aphrodite_min_p = 0.05
aphrodite_model =
#
max_tokens = 4096
local_api_timeout = 90
local_api_retries = 3
local_api_retry_delay = 5
streaming = True
temperature = 0.7
top_p = 0.9
min_p = 0.05
# https://artefact2.github.io/llm-sampling/

[Processing]
processing_choice = cuda

[Settings]
# Rename to LLM_API_Settings
chunk_duration = 30
words_per_second = 3
save_character_chats = False
save_rag_chats = False

[Auto-Save]
save_character_chats = False
save_rag_chats = False

[TTS-Settings]
# General TTS Settings
default_tts_provider = openai
default_tts_voice = shimmer
default_tts_speed = 1
#
# OpenAI TTS Settings
# available voices are 'alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer'
default_openai_tts_voice = shimmer
default_openai_tts_speed = 1
# available models are 'tts-1' or 'tts-1-hd'
default_openai_tts_model = tts-1-hd
default_openai_tts_output_format = mp3
#
# ElevenLabs TTS Settings
default_eleven_tts_voice = pNInz6obpgDQGcFmaJgB
default_eleven_tts_model = FIXME
default_eleven_tts_language_code = FIXME
default_eleven_tts_voice_stability = FIXME
default_eleven_tts_voice_similiarity_boost = FIXME
default_eleven_tts_voice_style = FIXME
default_eleven_tts_voice_use_speaker_boost = FIXME
default_eleven_tts_voice_pronunciation_dictionary_locators_dict_id = FIXME
default_eleven_tts_voice_pronunciation_dictionary_locators_version_id = FIXME
default_eleven_tts_speed = 1
default_eleven_tts_output_format = FIXME
# Google TTS Settings
default_google_tts_model = FIXME
default_google_tts_voice = FIXME
default_google_tts_speed = 1
#
# MS Edge TTS Settings
edge_tts_voice = FIXME
#

[Search-Engines]
# Search Defaults
search_provider_default = google
search_language_query = en
search_language_results = en
search_language_analysis = en
search_default_max_queries = 10
search_enable_subquery = True
search_enable_subquery_count_max = 5
search_result_rerank = True
search_result_max = 15
search_result_max_per_query = 10
search_result_blacklist = []
search_result_display_type = list
search_result_display_metadata = False
search_result_save_to_db = True
# How you want the results to be written, think 'style' or voice
search_result_analysis_tone =
#### Search Engines #####
# Baidu
search_engine_api_key_baidu = <baidu_api_key>>
#
# Bing
search_engine_api_url_bing = https://api.bing.microsoft.com/v7.0/search
search_engine_api_key_bing = <bing_api_key>
search_engine_country_code_bing = en
#
# Brave
search_engine_api_key_brave_regular = <brave_api_key>
search_engine_api_key_brave_ai = <brave_ai_api_key>
search_engine_country_code_brave = US
#
# DuckDuckGo
#
# Google
search_engine_api_url_google = https://www.googleapis.com/customsearch/v1?
search_engine_api_key_google = <google_api_key>
search_engine_id_google = <google_search_engine_id>
# 0 = Enable / 1 = Disabled
enable_traditional_chinese = 0
# Restricts search results to documents originating in a particular country.
limit_google_search_to_country = False
google_search_country_code = US
google_filter_setting = 1
google_user_geolocation = US
google_ui_language = en
google_limit_search_results_to_language =
google_default_search_results =
google_safe_search = "active"
google_enable_site_search =
google_site_search_include =
google_site_search_exclude =
# https://developers.google.com/custom-search/docs/structured_search#sort-by-attribute
google_sort_results_by =
#
# Kagi
search_engine_api_key_kagi = <kagi_api_key>>
# SearX
search_engine_searx_api = https://search.rhscz.eu/
# Serper
# Tavily
search_engine_api_key_tavily = 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
# Yandex
search_engine_api_key_yandex = <yandex_api_key>>
search_engine_id_yandex = <yandex_search_engine_id>>

[Prompts]
prompt_sample = "What is the meaning of life?"
video_summarize_prompt = "Above is the transcript of a video. Please read through the transcript carefully. Identify the main topics that are discussed over the course of the transcript. Then, summarize the key points about each main topic in bullet points. The bullet points should cover the key information conveyed about each topic in the video, but should be much shorter than the full transcript. Please output your bullet point summary inside <bulletpoints> tags. Do not repeat yourself while writing the summary."


[Database]
type = sqlite
sqlite_path = Databases/media_summary.db
backup_path = ./tldw_DB_Backups/
#Path to the backup location for the database. If the path does not exist, the backup will not be created.
elasticsearch_host = localhost
elasticsearch_port = 9200
# Additionally you can use elasticsearch as the database type, just replace `sqlite` with `elasticsearch` for `type` and provide the `elasticsearch_host` and `elasticsearch_port` of your configured ES instance.
chroma_db_path = Databases/chroma_db
prompts_db_path = Databases/prompts.db
rag_qa_db_path = Databases/RAG_QA_Chat.db
character_db_path = Databases/chatDB.db

[Embeddings]
embedding_provider = openai
embedding_model = text-embedding-3-small
onnx_model_path = ./App_Function_Libraries/models/onnx_models/
model_dir = ./App_Function_Libraries/models/embedding_models
embedding_api_url = http://localhost:8080/v1/embeddings
embedding_api_key = your_api_key_here
chunk_size = 400
overlap = 200
# 'embedding_provider' Can be 'openai', 'local', or 'huggingface'
# `embedding_model` Set to the model name you want to use for embeddings. For OpenAI, this can be 'text-embedding-3-small', or 'text-embedding-3-large'.
# huggingface: model = dunzhang/stella_en_400M_v5

[Chunking]
method = words
# 'method' Can be 'words' / 'sentences' / 'paragraphs' / 'semantic' / 'tokens'
max_size = 400
overlap = 200
adaptive = false
# Use ntlk+punkt to split text into sentences and then ID average sentence length and set that as the chunk size
multi_level = false
language = english

[Metrics]
log_file_path =
#os.getenv("tldw_LOG_FILE_PATH", "tldw_app_logs.json")
max_bytes =
#int(os.getenv("tldw_LOG_MAX_BYTES", 10 * 1024 * 1024))  # 10 MB
backup_count = 5
#int(os.getenv("tldw_LOG_BACKUP_COUNT", 5))

#[Comments]
#OpenAI Models:
#    gpt-4o
#    gpt-4o-2024-08-06
#    gpt-4o-mini
#    o1-preview
#    o1-mini
#    text-embedding-3-large
#    text-embedding-3-small
#
#Anthropic Models:
#    claude-3-5-sonnet-20241022
#    claude-3-5-sonnet-20240620
#    claude-3-5-haiku-20241022
#    claude-3-opus-20240229
#
#Cohere Models:
#    command-r-plus-08-2024
#    command-r-plus-04-2024
#    command-r-08-2024
#    command-r-03-2024
#
#DeepSeek Models:
#    deepseek-chat
#
#Groq Models:
#    f
#Mistral Models:
#    mistral-large-latest
#    open-mistral-nemo
#    codestral-latest
#    mistral-embed
#    open-mistral-7b
#    open-mixtral-8x7b
#    open-mixtral-8x22b
#    open-codestral-mamba
# Google's Models (11/15/2024): https://ai.google.dev/gemini-api/docs/models/gemini
#   gemini-1.5-pro
#   gemini-1.5-pro-2
#   LearnLM
#   gemini-1.5-flash
#   gemini-1.5-flash-8b
#   aqa
#   text-embedding-004