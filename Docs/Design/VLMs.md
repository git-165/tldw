# Vision-Language Models

## Introduction


### Link Dump:
https://arxiv.org/abs/2411.18279
https://github.com/breezedeus/Pix2Text
https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct
https://huggingface.co/tencent/HunyuanVideo
https://huggingface.co/OpenGVLab/InternVL2_5-38B
https://ivy-lvlm.github.io/Video-MA2MBA/

https://huggingface.co/Qwen/Qwen2-VL-72B
https://huggingface.co/Qwen/Qwen2-VL-7B
https://huggingface.co/Qwen/Qwen2-VL-2B

https://www.reddit.com/r/StableDiffusion/comments/1h7hunp/how_to_run_hunyuanvideo_on_a_single_24gb_vram_card/
https://arxiv.org/abs/2412.05185
https://huggingface.co/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c

https://huggingface.co/papers/2412.07626
https://huggingface.co/AI-Safeguard/Ivy-VL-llava
https://github.com/matatonic/openedai-vision
https://github.com/breezedeus/Pix2Text
https://github.com/Tencent/HunyuanVideo
https://github.com/huggingface/blog/blob/main/smolvlm.md
https://huggingface.co/tencent/HunyuanVideo
https://aivideo.hunyuan.tencent.com/
https://github.com/deepseek-ai/DeepSeek-VL2
https://arxiv.org/abs/2409.17146



